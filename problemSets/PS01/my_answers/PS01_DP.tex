\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 11, 2026}
\author{Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Wednesday February 11, 2026. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:

\begin{lstlisting}[language=R]
	# create empirical distribution of observed data
	ECDF <- ecdf(data)
	empiricalCDF <- ECDF(data)
	# generate test statistic
	D <- max(abs(empiricalCDF - pnorm(data))) \end{lstlisting}
\newpage
\section*{Solution - Problem 1} 
\vspace{.5cm}
\noindent The function below translates the formula for the p-value calculated from the Kolmogorov–Smirnov CDF into R code. Since each term in the final sum decreases as its index k increases (because the terms are of the form e to a negative power and e is greater than 1), raising them to increasingly negative powers causes the terms to converge to 0. To ensure that the function has a stopping point (and does not run infinitely), I set a \text{max\_k} value. The chosen value of 1000 is arbitrary, but sufficiently large to ensure that any additional terms beyond this point would not substantially change the resulting p-value and its interpretation. 

\medskip
 \noindent As the individual terms are calculated, they are stored in a vector, after which their sum is computed. This sum is then used in the final formula that returns the desired p-value.
 \vspace{.25cm}
\lstinputlisting[language=R, firstline=39,lastline=62]{PS01_DP.R} 
	\begin{verbatim}
	[1] 5.652523e-29
\end{verbatim}
\noindent The obtained p-value is much smaller than the 0.05 significance threshold, so we can reject the null hypothesis. Since this is a Kolmogorov–Smirnov test, the null hypothesis is that the data were sampled from the same distribution as the reference distribution. This result checks out, given that our empirical distribution consists of 1,000 Cauchy random variables and the reference distribution is normal. Because the Cauchy distribution is different from the normal distribution (it can have heavier tails), the conducted KS test detects this dissimilarity and allows us to reject the null hypothesis.
\newpage
\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}
\lstinputlisting[language=R, firstline=69,lastline=71]{PS01_DP.R} 
\vspace{.5cm}
\section*{Solution - Problem 2} 
Before running the regression, we will take a look at the data to make sure the model choice makes sense: 
\lstinputlisting[language=R, firstline=74,lastline=74]{PS01_DP.R} 
	\begin{figure}[h!]\centering
	\caption{\footnotesize scatterplot of X and Y}
	\label{fig:plot_1}
	\includegraphics[width=.82\textwidth]{scatter_xy.pdf}
\end{figure} 
\newpage
\lstinputlisting[language=R, firstline=81,lastline=98]{PS01_DP.R} 
\begin{verbatim}
Intercept: 0.13917 
Slope: 2.726695 
\end{verbatim}
\lstinputlisting[language=R, firstline=78,lastline=79]{PS01_DP.R} 
\begin{verbatim}
	Coefficients:
	Estimate Std. Error t value Pr(>|t|)    
	(Intercept)  0.13919    0.25276   0.551    0.582    
	x            2.72670    0.04159  65.564   <2e-16 ***
	---
	Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end{verbatim}
\noindent The main difference between the MLE and OLS approaches lies in the assumptions they make. In the MLE framework, we have to assume a specific probability distribution for the outcome variable. For a linear model, we assume a normal distribution with mean \(X\beta\) and variance \(\sigma^{2}\). The MLE approach then estimates the coefficients \(\beta\) and the variance \(\sigma^{2}\) by maximising the likelihood function, so by finding the values that make the observed data most probable. This is done by constructing the log-likelihood function for \(\beta\) and \(\sigma\) and optimising it using the Newton-Raphson algorithm.

\medskip
\noindent In contrast, the ordinary least squares (OLS) method, implemented in the \texttt{lm} function, estimates \(\beta\) by minimising the sum of squared residuals. While OLS does not require a specific distribution for the outcome, it does assume that the errors are normally distributed. For a linear model that fulfils this assumption, OLS is equivalent to MLE. This explains why the coefficients obtained using \texttt{lm} and those obtained by optimising the log-likelihood function using \texttt{optim} are very close in value, as we expected.
\end{document}
